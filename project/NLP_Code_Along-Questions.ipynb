{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Code Along questions\n",
    "\n",
    "For this code along we will build a spam filter!\n",
    "\n",
    "We'll use a classic dataset for this - UCI Repository SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and  read the dataset,  have Spark infer the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.functions import col, udf, length\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(\"SMSSpamCollection\",inferSchema=True,sep='\\t')\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new length feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+\n",
      "| _c0|                 _c1|length|\n",
      "+----+--------------------+------+\n",
      "| ham|Go until jurong p...|   111|\n",
      "| ham|Ok lar... Joking ...|    29|\n",
      "|spam|Free entry in 2 a...|   155|\n",
      "| ham|U dun say so earl...|    49|\n",
      "| ham|Nah I don't think...|    61|\n",
      "|spam|FreeMsg Hey there...|   147|\n",
      "| ham|Even my brother i...|    77|\n",
      "| ham|As per your reque...|   160|\n",
      "|spam|WINNER!! As a val...|   157|\n",
      "|spam|Had your mobile 1...|   154|\n",
      "| ham|I'm gonna be home...|   109|\n",
      "|spam|SIX chances to wi...|   136|\n",
      "|spam|URGENT! You have ...|   155|\n",
      "| ham|I've been searchi...|   196|\n",
      "| ham|I HAVE A DATE ON ...|    35|\n",
      "|spam|XXXMobileMovieClu...|   149|\n",
      "| ham|Oh k...i'm watchi...|    26|\n",
      "| ham|Eh u remember how...|    81|\n",
      "| ham|Fine if thats th...|    56|\n",
      "|spam|England v Macedon...|   155|\n",
      "+----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.withColumn('length', length(dataset['_c1']))\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the groupy mean of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| _c0|      avg(length)|\n",
      "+----+-----------------+\n",
      "| ham|71.45431945307645|\n",
      "|spam|138.6706827309237|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.groupby('_c0').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you transform you raw text in to tf_idf model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chain the transformer Tokenizer, StopWordsRemover, CountVectorizer and IDF for text to have a final column name 'tf_idf'\n",
    "- use the transformer StringIndexer for class column into output column 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create feature with vector assembler 'tf_idf','length of as input columns into output column named 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"_c1\", outputCol=\"_c1_token\")\n",
    "remover = StopWordsRemover(inputCol=\"_c1_token\", outputCol=\"_c1_filtered\")\n",
    "cv = CountVectorizer(inputCol=\"_c1_filtered\", outputCol=\"_c1_cv\")\n",
    "idf = IDF(inputCol=\"_c1_cv\", outputCol=\"tf_idf\")\n",
    "si = StringIndexer(inputCol='_c0',outputCol='label')\n",
    "va = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pipeline for fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: it may differ for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,139...|\n",
      "|  0.0|(13424,[10,53,103...|\n",
      "|  0.0|(13424,[125,184,4...|\n",
      "|  1.0|(13424,[1,47,118,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,120...|\n",
      "|  1.0|(13424,[8,17,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,96,217...|\n",
      "|  0.0|(13424,[552,1697,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,47...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "p_line = Pipeline(stages=[tokenizer, remover, cv, idf, si, va])\n",
    "p_model = p_line.fit(dataset)\n",
    "clean_data = p_model.transform(dataset)\n",
    "clean_data = clean_data.select(['label','features'])\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect spam or Ham\n",
    "\n",
    "now use your tf-idf data to classify spam and ham\n",
    "\n",
    "feel free to use any classifier model\n",
    "\n",
    "result may differ for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = clean_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr_classifier = lr.fit(train)\n",
    "pred_lr = lr_classifier.transform(test)\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()\n",
    "nb_classifier = nb.fit(train)\n",
    "pred_nb = nb_classifier.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[19.2180739433199...|[0.99999999549498...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[28.4420722561174...|[0.99999999999955...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,78...|[30.6844364015617...|[0.99999999999995...|       0.0|\n",
      "|  0.0|(13424,[0,1,146,1...|[18.2972612834777...|[0.99999998868641...|       0.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[18.2628798756859...|[0.99999998829067...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[13.0310820280527...|[0.99999780885039...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[13.1081856910984...|[0.99999797144678...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,6...|[56.1195717396599...|[1.0,4.2420879335...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,3...|[19.6874678011734...|[0.99999999718264...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[46.9524396381249...|[1.0,4.0626981211...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,7...|[64.8053502679233...|[1.0,7.1679337194...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[60.0052437425067...|[1.0,8.7107140530...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,25,...|[26.0160761900993...|[0.99999999999497...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,128...|[20.3244416844555...|[0.99999999850992...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,31,...|[32.6261581118152...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,43,...|[48.5233248663204...|[1.0,8.4447688575...|       0.0|\n",
      "|  0.0|(13424,[0,2,13,31...|[17.4872930336372...|[0.99999997456890...|       0.0|\n",
      "|  0.0|(13424,[0,2,20,10...|[31.0833116474428...|[0.99999999999996...|       0.0|\n",
      "|  0.0|(13424,[0,2,21,27...|[13.7035130825041...|[0.99999888149121...|       0.0|\n",
      "|  0.0|(13424,[0,2,23,31...|[39.8921653368022...|[1.0,4.7320870268...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[-610.79815822679...|[1.0,2.3686196747...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[-450.65668202381...|[1.0,8.3598469825...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,78...|[-689.69780596248...|[1.0,5.6344961383...|       0.0|\n",
      "|  0.0|(13424,[0,1,146,1...|[-252.84380660161...|[0.89357363744565...|       0.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[-321.30704753453...|[0.99999999999645...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-95.878781989564...|[0.99999998533590...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-97.551562867474...|[0.99999998847918...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,6...|[-2570.6773492518...|[1.0,2.8143697323...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,3...|[-489.24566631553...|[1.0,4.8787732690...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3317.2542820093...|[1.0,9.0724288069...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,7...|[-986.82185664418...|[1.0,1.3842809555...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-1620.0098235107...|[1.0,2.0794432853...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,25,...|[-420.33770511523...|[1.0,3.9582241374...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,128...|[-642.00445643089...|[1.0,9.5958629176...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,31,...|[-658.91723982030...|[1.0,6.2592692321...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,43,...|[-593.39592621557...|[1.0,2.3919673713...|       0.0|\n",
      "|  0.0|(13424,[0,2,13,31...|[-768.77059841737...|[1.0,7.7803105918...|       0.0|\n",
      "|  0.0|(13424,[0,2,20,10...|[-429.39544350929...|[1.0,3.6271454239...|       0.0|\n",
      "|  0.0|(13424,[0,2,21,27...|[-700.01322100921...|[1.0,1.0771900770...|       0.0|\n",
      "|  0.0|(13424,[0,2,23,31...|[-889.15960692267...|[1.0,4.5344957075...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression: 0.9657210192437184\n",
      "Accuracy of Naive Bayes: 0.9213967408750166\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       969\n",
      "         1.0       0.99      0.76      0.86       143\n",
      "\n",
      "    accuracy                           0.97      1112\n",
      "   macro avg       0.98      0.88      0.92      1112\n",
      "weighted avg       0.97      0.97      0.97      1112\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.95       969\n",
      "         1.0       0.60      0.99      0.75       143\n",
      "\n",
      "    accuracy                           0.91      1112\n",
      "   macro avg       0.80      0.95      0.85      1112\n",
      "weighted avg       0.95      0.91      0.92      1112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "mce = MulticlassClassificationEvaluator()\n",
    "\n",
    "accuracy_lr = mce.evaluate(pred_lr)\n",
    "print(\"Accuracy of Logistic regression: {}\".format(accuracy_lr))\n",
    "\n",
    "accuracy_nb = mce.evaluate(pred_nb)\n",
    "print(\"Accuracy of Naive Bayes: {}\".format(accuracy_nb))\n",
    "\n",
    "y_true_ = test.select(['label']).collect()\n",
    "y_pred_lr = pred_lr.select(['prediction']).collect()\n",
    "y_pred_nb = pred_nb.select(['prediction']).collect()\n",
    "\n",
    "print('\\n', classification_report(y_true, y_pred_lr))\n",
    "print(classification_report(y_true, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
